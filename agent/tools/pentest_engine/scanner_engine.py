"""Main vulnerability scanner engine."""

import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urljoin, urlparse
import re

from .http_client import HTTPClient
from .models import ScanResult, Vulnerability
from .ai.payload_generator import AIPayloadGenerator
from .modules.sql_injection import SQLInjectionScanner
from .modules.xss import XSSScanner
from .modules.other_vulns import (
    SSRFScanner,
    LFIScanner,
    CommandInjectionScanner,
    OpenRedirectScanner
)

logger = logging.getLogger(__name__)


class VulnerabilityScanner:
    """Main vulnerability scanning engine."""

    def __init__(
        self,
        target_url: str,
        verify_ssl: bool = False,
        proxies: Optional[Dict[str, str]] = None,
        custom_headers: Optional[Dict[str, str]] = None,
        max_threads: int = 5,
        max_depth: int = 2
    ):
        """Initialize scanner.

        Args:
            target_url: Target URL to scan
            verify_ssl: Whether to verify SSL certificates
            proxies: Proxy configuration
            custom_headers: Custom HTTP headers
            max_threads: Maximum concurrent threads
            max_depth: Maximum crawl depth
        """
        self.target_url = target_url
        self.max_threads = max_threads
        self.max_depth = max_depth

        # Initialize HTTP client
        self.http_client = HTTPClient(
            verify_ssl=verify_ssl,
            proxies=proxies,
            custom_headers=custom_headers
        )

        # Initialize AI payload generator
        self.payload_generator = AIPayloadGenerator()

        # Initialize vulnerability modules
        self.modules = [
            SQLInjectionScanner(self.http_client, self.payload_generator),
            XSSScanner(self.http_client, self.payload_generator),
            SSRFScanner(self.http_client, self.payload_generator),
            LFIScanner(self.http_client, self.payload_generator),
            CommandInjectionScanner(self.http_client, self.payload_generator),
            OpenRedirectScanner(self.http_client),
        ]

        # State
        self.visited_urls = set()
        self.discovered_urls = set()

    def scan(
        self,
        quick_scan: bool = False,
        crawl: bool = True,
        modules: Optional[List[str]] = None
    ) -> ScanResult:
        """Perform vulnerability scan.

        Args:
            quick_scan: If True, use fewer payloads and skip slow tests
            crawl: Whether to crawl the site for URLs
            modules: Specific modules to run (None = all)

        Returns:
            ScanResult object
        """
        logger.info(f"Starting scan of {self.target_url}")
        start_time = datetime.now()

        # Initialize result
        result = ScanResult(
            target=self.target_url,
            start_time=start_time
        )

        # Discover URLs
        if crawl:
            logger.info("Crawling for URLs...")
            self.discovered_urls = self._crawl(self.target_url, depth=0)
            logger.info(f"Discovered {len(self.discovered_urls)} URLs")
        else:
            self.discovered_urls = {self.target_url}

        result.urls_discovered = len(self.discovered_urls)

        # Filter modules
        active_modules = self.modules
        if modules:
            active_modules = [m for m in self.modules if m.name.lower() in [n.lower() for n in modules]]

        # Scan URLs
        logger.info(f"Scanning {len(self.discovered_urls)} URLs with {len(active_modules)} modules...")

        for url in self.discovered_urls:
            logger.info(f"Scanning URL: {url}")

            # Get page context for AI
            context = self._get_page_context(url)

            # Run each module
            for module in active_modules:
                try:
                    logger.debug(f"Running {module.name} on {url}")
                    vulns = module.scan(url, context=context, quick=quick_scan)

                    for vuln in vulns:
                        result.add_vulnerability(vuln)
                        logger.warning(f"Found {vuln.type.value}: {vuln.description[:100]}")

                except Exception as e:
                    logger.error(f"Error in {module.name} for {url}: {e}")

            result.urls_scanned += 1

        # Finalize result
        result.end_time = datetime.now()
        result.scan_duration = (result.end_time - result.start_time).total_seconds()

        logger.info(f"Scan complete. Found {len(result.vulnerabilities)} vulnerabilities.")
        logger.info(f"Summary: {result.get_summary()}")

        return result

    def _crawl(self, url: str, depth: int) -> set:
        """Crawl website to discover URLs.

        Args:
            url: Starting URL
            depth: Current depth

        Returns:
            Set of discovered URLs
        """
        if depth > self.max_depth:
            return set()

        if url in self.visited_urls:
            return set()

        self.visited_urls.add(url)
        discovered = {url}

        # Fetch page
        response = self.http_client.get(url)
        if not response:
            return discovered

        # Extract links
        links = self._extract_links(response.text, url)

        # Filter to same domain
        base_domain = urlparse(self.target_url).netloc
        same_domain_links = [
            link for link in links
            if urlparse(link).netloc == base_domain
        ]

        # Recursively crawl
        for link in same_domain_links[:20]:  # Limit to 20 links per page
            if link not in self.visited_urls:
                discovered.update(self._crawl(link, depth + 1))

        return discovered

    @staticmethod
    def _extract_links(html: str, base_url: str) -> List[str]:
        """Extract links from HTML.

        Args:
            html: HTML content
            base_url: Base URL for resolving relative links

        Returns:
            List of absolute URLs
        """
        # Simple regex-based link extraction
        link_pattern = re.compile(r'href=["\'](.*?)["\']', re.IGNORECASE)
        links = link_pattern.findall(html)

        absolute_links = []
        for link in links:
            # Skip anchors, javascript, mailto, etc.
            if link.startswith(('#', 'javascript:', 'mailto:', 'tel:')):
                continue

            # Convert to absolute URL
            absolute_url = urljoin(base_url, link)

            # Validate
            if HTTPClient.is_valid_url(absolute_url):
                absolute_links.append(absolute_url)

        return list(set(absolute_links))  # Remove duplicates

    def _get_page_context(self, url: str, max_length: int = 500) -> str:
        """Get page context for AI payload generation.

        Args:
            url: URL to fetch
            max_length: Maximum context length

        Returns:
            Page context string
        """
        try:
            response = self.http_client.get(url)
            if response:
                # Extract text content (simple approach)
                text = re.sub(r'<[^>]+>', ' ', response.text)  # Remove HTML tags
                text = re.sub(r'\s+', ' ', text)  # Normalize whitespace
                return text[:max_length]
        except Exception as e:
            logger.debug(f"Error getting context for {url}: {e}")

        return ""
